{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spell checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "#Importing the necessary libraries\n",
    "%pylab inline\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "from collections import Counter\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining functions that run one into the other leading up to a set corrections for a mispelt word (CANDIDATES function) as well as the most probable correction for the mistake (CORRECTION function) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Counter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9495410566a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'\\w+'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mWORDS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'big.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mwordcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Counter' is not defined"
     ]
    }
   ],
   "source": [
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('big.txt').read()))\n",
    "\n",
    "wordcount = []\n",
    "for value in WORDS.values():\n",
    "    wordcount.append(value)\n",
    "\n",
    "total = sum(wordcount)\n",
    "\n",
    "def P(word): \n",
    "    N = total\n",
    "    \"Probability of `word`.\"\n",
    "    return (WORDS[word] / N)\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CANDIDATES function - example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates('helo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORRECTION function - example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correction(\"helo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis on bag of words ('big.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just some EDA to explore the file we use as bag of words = \n",
    "WORDS.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#most popular word\n",
    "max(WORDS, key=P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probability of the word the appearing in the WORDS file (\"big.txt\")\n",
    "P('the')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer function - listing words separated by a space as well as turning everything to lowercase (NORMALIZING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens(text):\n",
    "    \"List all the word tokens (consecutive letters) in a text. Normalize to lowercase.\"\n",
    "    return re.findall('[a-z]+', text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens(\"I like Big BUTS and i cannot lie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTS = Counter(WORDS)\n",
    "COUNTS['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in tokens('the rare and neverbeforeseen words'):\n",
    "    print (COUNTS[w], w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer additional feature + example - used for words that are not separated by a space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordsegment\n",
    "from wordsegment import segment\n",
    "\n",
    "segment('thisisatest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPLITS function + example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splits(word):\n",
    "    \"Return a list of all possible (first, rest) pairs that comprise word.\"\n",
    "    return [(word[:i], word[i:]) \n",
    "            for i in range(len(word)+1)]\n",
    "\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits('neverseen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORRECT function + example - finding the best spelling correction for any given word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def known(words):\n",
    "    \"Return the subset of words that are actually in the dictionary.\"\n",
    "    return {w for w in words if w in COUNTS}\n",
    "\n",
    "def edits0(word): \n",
    "    \"Return all strings that are zero edits away from word (i.e., just word itself).\"\n",
    "    return {word}\n",
    "\n",
    "def edits1(word):\n",
    "    \"Return all strings that are one edit away from this word.\"\n",
    "    pairs      = splits(word)\n",
    "    deletes    = [a+b[1:]           for (a, b) in pairs if b]\n",
    "    transposes = [a+b[1]+b[0]+b[2:] for (a, b) in pairs if len(b) > 1]\n",
    "    replaces   = [a+c+b[1:]         for (a, b) in pairs for c in alphabet if b]\n",
    "    inserts    = [a+c+b             for (a, b) in pairs for c in alphabet]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word):\n",
    "    \"Return all strings that are two edits away from this word.\"\n",
    "    return {e2 for e1 in edits1(word) for e2 in edits1(e1)}\n",
    "\n",
    "def correct(word):\n",
    "    \"Find the best spelling correction for this word.\"\n",
    "    # Prefer edit distance 0, then 1, then 2; otherwise default to word itself.\n",
    "    candidates = (known(edits0(word)) or \n",
    "                  known(edits1(word)) or \n",
    "                  known(edits2(word)) or \n",
    "                  [word])\n",
    "    return max(candidates, key=COUNTS.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct('helo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORRECT_TEXT fucntion + example - building up from the CORRECT function where now we try to spell check sentences and not single words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_text(text):\n",
    "    \"Correct all the words within a text, returning the corrected text.\"\n",
    "    return re.sub('[a-zA-Z]+', correct_match, text)\n",
    "\n",
    "def correct_match(match):\n",
    "    \"Spell-correct word in match, and preserve proper upper/lower/title case.\"\n",
    "    word = match.group()\n",
    "    return case_of(word)(correct(word.lower()))\n",
    "\n",
    "def case_of(text):\n",
    "    \"Return the case-function appropriate for text: upper, lower, title, or just str.\"\n",
    "    return (str.upper if text.isupper() else\n",
    "            str.lower if text.islower() else\n",
    "            str.title if text.istitle() else\n",
    "            str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_text('Speling errurs in somethink. Whutever; unusuel misteakes everyware?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDIST function + example - finding the probability of occurrence of a given word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcount2 = []\n",
    "for value in COUNTS.values():\n",
    "    wordcount2.append(value)\n",
    "\n",
    "total2 = sum(wordcount2)\n",
    "\n",
    "def pdist(word): \n",
    "    N2 = total2\n",
    "    \"Probability of `word`.\"\n",
    "    return lambda x: word[x] / N2\n",
    "\n",
    "P = pdist(COUNTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in tokens('\"The\" is most common word in English'):\n",
    "    print (P(w), w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PWORDS function + example - finding the probability of occurrence of a given word, assuming each word selection independence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pwords(words):\n",
    "    \"Probability of words, assuming each word is independent of others.\"\n",
    "    return product(P(w) for w in words)\n",
    "\n",
    "def product(nums):\n",
    "    \"Multiply the numbers together.  (Like `sum`, but with multiplication.)\"\n",
    "    result = 1\n",
    "    for x in nums:\n",
    "        result *= x\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = ['this is a test', \n",
    "         'this is a unusual test',\n",
    "         'this is a neverbeforeseen test']\n",
    "\n",
    "for test in tests:\n",
    "    print (Pwords(tokens(test)), test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build up to probabilistic functions PWORDS, CPWORDS & PWORDS2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_counts(filename, sep='\\t'):\n",
    "    \"\"\"Return a Counter initialized from key-value pairs, \n",
    "    one on each line of filename.\"\"\"\n",
    "    C = Counter()\n",
    "    for line in open(filename):\n",
    "        key, count = line.split(sep)\n",
    "        C[key] = int(count)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTS1 = load_counts('count_1w.txt')\n",
    "COUNTS2 = load_counts('count_2w.txt')\n",
    "\n",
    "P1w = pdist(COUNTS1)\n",
    "P2w = pdist(COUNTS2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PWORDS = probability of word occurrence assuming word selection independence\n",
    "\n",
    "# CPWORDS = conditional probability - occurrence of one word given the presence of the previous one\n",
    "\n",
    "# PWORDS2 = using bigram data, outputs probability of a sequence of words given previous words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pwords2(words, prev='<S>'):\n",
    "    \"The probability of a sequence of words, using bigram data, given prev word.\"\n",
    "    return product(cPword(w, (prev if (i == 0) else words[i-1]) )\n",
    "                   for (i, w) in enumerate(words))\n",
    "\n",
    "# Change Pwords to use P1w (the bigger dictionary) instead of Pword\n",
    "def Pwords(words):\n",
    "    \"Probability of words, assuming each word is independent of others.\"\n",
    "    return product(P1w(w) for w in words)\n",
    "\n",
    "def cPword(word, prev):\n",
    "    \"Conditional probability of word, given previous word.\"\n",
    "    bigram = prev + ' ' + word\n",
    "    if P2w(bigram) > 0 and P1w(prev) > 0:\n",
    "        return P2w(bigram) / P1w(prev)\n",
    "    else: # Average the back-off value and zero.\n",
    "        return P1w(word) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interface():\n",
    "    a = input(\"enter some text: \")\n",
    "    b = a \n",
    "    print(\"Spell-checked version:/n\")\n",
    "    return correct_text(b)\n",
    "\n",
    "interface()\n",
    "\n",
    "#Your sir, arew notf very nicew, you never pasd me the ball when I ask for it\n",
    "#look at how it corrected pasd to past. It did not take into accout keyboard proximity. You need to add that feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigrams, Bigrams + NLTK Ngrams example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N-grams\n",
    "\n",
    "import wordsegment as ws\n",
    "ws.load()\n",
    "ws.UNIGRAMS['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<s> means starts with\n",
    "ws.BIGRAMS['<s> where']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "from pprint import pprint\n",
    "from operator import itemgetter\n",
    "pprint(heapq.nlargest(10, ws.BIGRAMS.items(), itemgetter(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLTK n-grams, so that if we want we can tweak n and place whatever number we want. \n",
    "\n",
    "from nltk import ngrams\n",
    "sentence = 'this is a foo bar sentences and i want to ngramize it'\n",
    "n = 3\n",
    "sixgrams = ngrams(sentence.split(), n)\n",
    "for grams in sixgrams:\n",
    "  print (grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitter(text):\n",
    "    wordlist = list(text) \n",
    "    wordlist = ' '.join(wordlist).split()\n",
    "    return wordlist\n",
    "\n",
    "splitter(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = list(candidates(\"helo\"))\n",
    "\n",
    "\n",
    "for i in b:\n",
    "    b = (i.split(','))\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KEYBOARD_DISTANCE function + example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "keyboard_cartesian = {'q': {'x':0, 'y':0}, 'w': {'x':1, 'y':0}, 'e': {'x':2, 'y':0}, 'r': {'x':3, 'y':0}, 't': {'x':4, 'y':0}, 'y': {'x':5, 'y':0}, 'u': {'x':6, 'y':0}, 'i': {'x':7, 'y':0}, 'o': {'x':8, 'y':0}, 'p': {'x':9, 'y':0}, 'a': {'x':0, 'y':1},'z': {'x':0, 'y':2},'s': {'x':1, 'y':1},'x': {'x':1, 'y':2},'d': {'x':2, 'y':1},'c': {'x':2, 'y':2}, 'f': {'x':3, 'y':1}, 'b': {'x':4, 'y':2}, 'm': {'x':5, 'y':2}, 'j': {'x':6, 'y':1}, 'g': {'x':4, 'y':1}, 'h': {'x':5, 'y':1}, 'j': {'x':6, 'y':1}, 'k': {'x':7, 'y':1}, 'l': {'x':8, 'y':1}, 'v': {'x':3, 'y':2}, 'n': {'x':5, 'y':2}, }\n",
    "\n",
    "def keyboard_distance(a,b):\n",
    "    X = (keyboard_cartesian[a]['x'] - keyboard_cartesian[b]['x'])**2\n",
    "    Y = (keyboard_cartesian[a]['y'] - keyboard_cartesian[b]['y'])**2\n",
    "    return math.sqrt(X+Y) \n",
    "     \n",
    "keyboard_distance('q', 'p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEVENSHTEIN distance + example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein as lev\n",
    "\n",
    "word1 = \"focus\"\n",
    "word2 = \"ficus\"\n",
    "\n",
    "lev.ratio(word1,word2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOUNDEX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#soundex\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "fuzz.ratio(\"ACME Factory\", \"ACME Factory Inc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenized SOUNDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzz.token_sort_ratio('Barack Obama', 'Barack H. Obama')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING - \n",
    "A process applied to the functions we developed. We will come up with a logistic regression as to analyze two distinct but deeply intertwined metrics:\n",
    "Firstly, we analyze the coefficients of this binary classifier as to understand what weights are to be applied to the separte functions we created once they'll all be joined in a unique, fully-fledged application. \n",
    "Secondly, we compute the precision and recall of our model as to see how well the explanatory variable lead to the prediciton of our spell checker's performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let the machine learning start by loading the necessary files.\n",
    "import pandas as pd\n",
    "\n",
    "misspellings = pd.read_csv('/Users/filippofrezza/Desktop/IE/3rd term/NLP/data/misspellingg.csv',sep=';')\n",
    "misp_list = list(misspellings['misspellings'])\n",
    "misp_list = [element.lower() for element in misp_list]\n",
    "correct_answers = list(misspellings['correct_answers'])\n",
    "correct_answers = [element.lower() for element in correct_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coming up with the corrections for the list of misspelt words\n",
    "\n",
    "df = []\n",
    "for i in misp_list:\n",
    "    df.append(correct_text(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the initial dataframe\n",
    "\n",
    "#misspellings = misspelt words\n",
    "#correct_text = suggestion of our function for misspelt word\n",
    "#correct_answers = answers of truth set\n",
    "\n",
    "data = pd.DataFrame({'misspellings':misp_list,'correct_text':df,'correct_answers':correct_answers})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating binary variable as to see where our function - CORRECT_TEXT mathces the truth set = CORRECT_ANSWERS\n",
    "\n",
    "data['correct'] =data.apply(lambda row: row['correct_text'] == row['correct_answers'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building list of words to then insert in the tokenized soundex function\n",
    "\n",
    "word1 = list(data['misspellings'])\n",
    "word2 = list(data['correct_text'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the my_soundex column\n",
    "\n",
    "my_soundex = []\n",
    "\n",
    "for i in range(len(word1)):\n",
    "    my_soundex.append(fuzz.token_sort_ratio(word1[i],word2[i]))\n",
    "    \n",
    "data['my_soundex'] = my_soundex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#updating the dataframe\n",
    "\n",
    "data = data[['correct_answers', 'correct_text', 'misspellings', 'my_levkey','my_soundex','correct']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this transforms the correct column previously created from TRUE & FALSE to 1s & 0s\n",
    "data['correct'] = data['correct'] * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dictionary as to display all the solutions (CANDIDATES function) for a given misspelt word\n",
    "\n",
    "candidates_dict = {}\n",
    "for i in range(len(misp_list)):\n",
    "    candidates_dict[misp_list[i]] = candidates(misp_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECKER function + example - it checks whether the most the CORRECT_ANSWER for a misspelt word is within the list of words derived from our CANDIDATES function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checker(misspell,correct):\n",
    "    x = 0\n",
    "    if misspell in candidates_dict.keys():\n",
    "        misspell = str(misspell.lower())\n",
    "        if correct in list(candidates_dict[misspell]):\n",
    "            x = 1\n",
    "            return(\"1\")\n",
    "        else:\n",
    "            x = 0\n",
    "            return(\"0\")\n",
    "    else:\n",
    "        x = None\n",
    "        return(\"2\")\n",
    "    \n",
    "checker(\"circue\",\"circle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the candidates (binary variable) column in the dataframe\n",
    "\n",
    "candidates_l = []\n",
    "for i in range(len(misp_list)):\n",
    "    candidates_l.append(checker(misp_list[i],correct_answers[i]))\n",
    "\n",
    "data['candidates'] = candidates_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build-up of weighted keyboard distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a list where we take the correct answer if our candidates function has the correct answer in its output.\n",
    "#If not, we append the first solution of our candidates function. \n",
    "\n",
    "candidate_list = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if data['candidates'][i] == '1':\n",
    "        candidate_list.append(data['correct_answers'][i])\n",
    "    else:\n",
    "        candidate_list.append(list(candidates(misp_list[i]))[0])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making words of equal length\n",
    "\n",
    "candidate_list2 = candidate_list\n",
    "correct_answers2 = list(data['correct_answers'])\n",
    "\n",
    "for i in range(len(candidate_list2)):\n",
    "    if len(candidate_list2[i]) > len(correct_answers2[i]):\n",
    "        candidate_list2[i] = candidate_list2[i][:len(correct_answers2[i])]\n",
    "    if len(correct_answers2[i]) > len(candidate_list2[i]):\n",
    "        correct_answers2[i] = correct_answers2[i][:len(candidate_list2[i])]\n",
    "    else:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a dictionary as to register the keyboard distance of letters amongst different words.\n",
    "\n",
    "s = candidate_list2\n",
    "t = correct_answers2\n",
    "\n",
    "differences = {}\n",
    "\n",
    "for i in range(len(s)):\n",
    "    ss = candidate_list2[i]\n",
    "    tt = correct_answers2[i]\n",
    "    for j in range(len(ss)):\n",
    "        if ss[j] != tt[j]:\n",
    "            differences[misp_list[i]] = [ss[j],tt[j]]\n",
    "        else:\n",
    "            differences[misp_list[i]] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keyboard_distance(differences[j][0],differences[j][1])\n",
    "\n",
    "key_dist = []\n",
    "\n",
    "\n",
    "letters = [differences[x] for x in differences]\n",
    "\n",
    "for i in range(len(letters)):\n",
    "    try:\n",
    "        key_dist.append(keyboard_distance(letters[i][0],letters[i][1]))\n",
    "    except:\n",
    "        key_dist.append('0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the final output of this weighted keyboard distance process:\n",
    "\n",
    "word1 = misp_list\n",
    "word2 = list(data['correct_text'])\n",
    "\n",
    "my_levkey = []\n",
    "\n",
    "for i in range(len(misp_list)):\n",
    "    my_levkey.append(lev.ratio(word1[i],word2[i])*float(key_dist[i]))\n",
    "    \n",
    "data['my_levkey'] = my_levkey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just displaying how the created dataframe looks like\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression - dependent variable = CORRECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression - sci kit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_curve, auc\n",
    "from patsy import dmatrices\n",
    "\n",
    "df_train = data.iloc[ 0: 437, : ] \n",
    "df_test = data.iloc[ 437: , : ]\n",
    "formula = 'correct~my_levkey+my_soundex+candidates'\n",
    "y_train,x_train = dmatrices(formula, data=df_train, return_type='dataframe') \n",
    "y_test,x_test = dmatrices(formula, data=df_test, return_type='dataframe')\n",
    "\n",
    "model = LogisticRegression()\n",
    "model = model.fit(x_train, y_train.correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coefficient Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the coefficients\n",
    "pd.DataFrame(list(zip(x_train.columns, np.transpose(model.coef_))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision & Recall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#precision & recall\n",
    "\n",
    "y_pred = model.predict_proba(x_test)\n",
    "y_pred_flag = y_pred[:,1] > 0.5\n",
    "\n",
    "\n",
    "print (pd.crosstab(y_test.correct\n",
    "                  ,y_pred_flag\n",
    "                  ,rownames = ['Actual']\n",
    "                  ,colnames = ['Predicted']))\n",
    "\n",
    "print ('\\n \\n')\n",
    "\n",
    "print (classification_report(y_test,y_pred_flag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC curve + plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve and area the curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred[:,1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(\"Area under the ROC curve : %f\" % roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "plt.clf()\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
